{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b9d2a2d-5a57-41e3-9f9a-252667c1d45d",
   "metadata": {},
   "source": [
    "01. Implement sentimental analysis using RNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4678aad-81a7-4591-a4b7-808eab8e1bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 71ms/step - accuracy: 0.5451 - loss: 0.6847 - val_accuracy: 0.6815 - val_loss: 0.5851\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 65ms/step - accuracy: 0.7478 - loss: 0.5170 - val_accuracy: 0.7020 - val_loss: 0.5568\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 65ms/step - accuracy: 0.6970 - loss: 0.5755 - val_accuracy: 0.7514 - val_loss: 0.5395\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 67ms/step - accuracy: 0.8080 - loss: 0.4337 - val_accuracy: 0.7780 - val_loss: 0.4906\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 66ms/step - accuracy: 0.8107 - loss: 0.4372 - val_accuracy: 0.7004 - val_loss: 0.5853\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 65ms/step - accuracy: 0.8376 - loss: 0.3737 - val_accuracy: 0.8038 - val_loss: 0.4421\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 66ms/step - accuracy: 0.8811 - loss: 0.3054 - val_accuracy: 0.7892 - val_loss: 0.4913\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 66ms/step - accuracy: 0.8649 - loss: 0.3271 - val_accuracy: 0.7206 - val_loss: 0.6153\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 66ms/step - accuracy: 0.7829 - loss: 0.4556 - val_accuracy: 0.7296 - val_loss: 0.6265\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 64ms/step - accuracy: 0.7916 - loss: 0.4422 - val_accuracy: 0.6568 - val_loss: 0.6327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14450f3ddc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "max_len = 200\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(max_features, 128),\n",
    "    tf.keras.layers.SimpleRNN(128),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6dd171-3d9c-47e1-bb01-f62766d7082a",
   "metadata": {},
   "source": [
    "2.Write a code for splitting,training,Validation and test sets of Datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d127fce-7e25-4b03-98ea-3fc163270682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = range(1000)\n",
    "y = range(1000)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f55b7ee-e267-4e05-8be7-c054394cc118",
   "metadata": {},
   "source": [
    "3. Write a code to implement custom activation function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba4743e-5f31-4031-9209-2f868ba5f624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdabd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def custom_activation(x):\n",
    "    return K.sigmoid(x) * x\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, input_shape=(100,), activation=custom_activation),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89bd6f2-4ae5-4486-bcaf-fad2f17ef5dc",
   "metadata": {},
   "source": [
    "4. Implement a single LSTM cell from scratch. Create a forward pass for an LSTM network and test it on a simple sequential data task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde82cdc-75bc-4d3d-ad25-336792a18632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06984978 -0.14272195  0.21709209  0.60410815 -0.0546589   0.14733135\n",
      "   0.03683055  0.29187527]] [[-0.12189729 -0.63064732  0.69805743  0.71548868 -0.48616037  0.24229571\n",
      "   0.12778188  0.9702133 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LSTMCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "\n",
    "        self.bf = np.zeros((hidden_size, 1))\n",
    "        self.bi = np.zeros((hidden_size, 1))\n",
    "        self.bc = np.zeros((hidden_size, 1))\n",
    "        self.bo = np.zeros((hidden_size, 1))\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        combined = np.concatenate((h_prev, x), axis=1).T  # Transpose combined to (input_size + hidden_size, batch_size)\n",
    "        \n",
    "        forget_gate = self.sigmoid(np.dot(self.Wf, combined) + self.bf)\n",
    "        input_gate = self.sigmoid(np.dot(self.Wi, combined) + self.bi)\n",
    "        candidate_memory = np.tanh(np.dot(self.Wc, combined) + self.bc)\n",
    "        output_gate = self.sigmoid(np.dot(self.Wo, combined) + self.bo)\n",
    "        \n",
    "        c_next = forget_gate * c_prev.T + input_gate * candidate_memory\n",
    "        h_next = output_gate * np.tanh(c_next)\n",
    "        \n",
    "        return h_next.T, c_next.T  # Return h_next and c_next in their original shapes\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "input_size, hidden_size = 4, 8\n",
    "lstm_cell = LSTMCell(input_size, hidden_size)\n",
    "x = np.random.randn(1, input_size)  # Shape (1, input_size)\n",
    "h_prev = np.zeros((1, hidden_size))  # Shape (1, hidden_size)\n",
    "c_prev = np.zeros((1, hidden_size))  # Shape (1, hidden_size)\n",
    "\n",
    "h_next, c_next = lstm_cell.forward(x, h_prev, c_prev)\n",
    "print(h_next, c_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff729ec-9918-45fa-9ca6-8a246ddd7b4b",
   "metadata": {},
   "source": [
    "5. Write code to implement early stopping during the training process of a neural network. Track the validation loss and stop training when it increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53ef4d5e-d3d5-4cb7-86f8-b4d55f80f468",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(\u001b[43mx_val\u001b[49m, y_val), callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_val' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_val, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543549f5-edce-4341-bb63-8928d1db4b8c",
   "metadata": {},
   "source": [
    "6. Use a pretrained model (e.g., ResNet or VGG16) and fine-tune it for a specific task like classifying cats vs. dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb884ac6-bf02-4810-93f3-acebadd75e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    resnet,\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'C:/path/to/your/dataset',  \n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    'C:/path/to/your/dataset',  # Replace this with your dataset path\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "model.fit(train_generator, epochs=10, validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f50851-01bc-4f2e-a1eb-be69a3b566ed",
   "metadata": {},
   "source": [
    "7. Implement a simple RNN from scratch to generate sequences (e.g., text generation). Use a dataset like text sequences or simple number sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46d1dbdf-2f6a-41d4-a6bd-126855fa2c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdabd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 3098.4202\n",
      "Epoch 2/10\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2705.8894\n",
      "Epoch 3/10\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2562.6101\n",
      "Epoch 4/10\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2460.2893\n",
      "Epoch 5/10\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2329.6270\n",
      "Epoch 6/10\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2145.0813\n",
      "Epoch 7/10\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2070.8875\n",
      "Epoch 8/10\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1965.2521\n",
      "Epoch 9/10\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1911.6227\n",
      "Epoch 10/10\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1712.5945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1445db2e780>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "data = np.arange(0, 100, step=0.1)\n",
    "sequence_length = 10\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(data, sequence_length)\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(50, input_shape=(sequence_length, 1)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X, y, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c7def-c390-40d9-b0c4-45c122f103f4",
   "metadata": {},
   "source": [
    "8. Write a code to implement the sequential model using CNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a550c9c-ea39-40ca-a1a4-46f232fe1d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdabd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d597e0c-6eb7-4bec-8365-94b8543ac2d5",
   "metadata": {},
   "source": [
    "9. Write a code to implement the functional model using CNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d525e861-05b1-4300-b247-ffda66510630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_layer = Input(shape=(64, 64, 3))\n",
    "x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "output_layer = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8210dbb-8ad2-457c-837d-a208df113f38",
   "metadata": {},
   "source": [
    "10.Implement a simple Convolutional Neural Network (CNN) with one convolutional layer followed by pooling and fully connected layers. Use it on a small image dataset (e.g., CIFAR-10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a57a3-fbb1-44e5-be49-f2781850f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5306ee-3b91-4621-bf2a-c0d741c25336",
   "metadata": {},
   "source": [
    "11. Use a simple neural network with at least two hidden layers to classify handwritten digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53e20567-47a2-4640-b17d-32c1f872f3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8745 - loss: 0.4268 - val_accuracy: 0.9638 - val_loss: 0.1218\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9684 - loss: 0.1046 - val_accuracy: 0.9718 - val_loss: 0.0877\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9779 - loss: 0.0696 - val_accuracy: 0.9731 - val_loss: 0.0847\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9842 - loss: 0.0510 - val_accuracy: 0.9775 - val_loss: 0.0759\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9866 - loss: 0.0396 - val_accuracy: 0.9736 - val_loss: 0.0906\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9905 - loss: 0.0295 - val_accuracy: 0.9709 - val_loss: 0.1034\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9921 - loss: 0.0243 - val_accuracy: 0.9742 - val_loss: 0.0951\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9925 - loss: 0.0216 - val_accuracy: 0.9781 - val_loss: 0.0900\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9944 - loss: 0.0172 - val_accuracy: 0.9731 - val_loss: 0.1152\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9934 - loss: 0.0179 - val_accuracy: 0.9791 - val_loss: 0.0992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1445db28d10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(-1, 28*28) / 255.0\n",
    "x_test = x_test.reshape(-1, 28*28) / 255.0\n",
    "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(28*28,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0346ba8-8448-4316-8e81-9163bc9d23f2",
   "metadata": {},
   "source": [
    "12. Write a code to implement the batch size and epochs in the Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1784a-4c56-42ee-aaf0-732bdf39e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d0998-9ef7-4364-80d8-a3ca4d62eb93",
   "metadata": {},
   "source": [
    "13. Write a basic version of a Sequential model class similar to Keras API, where you can add layers and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e72d3f6e-6a72-4cae-9a64-299802502a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: Dense Layer 1\n",
      "Layer: Dense Layer 2\n"
     ]
    }
   ],
   "source": [
    "class SequentialModel:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.compiled = False\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def compile(self, optimizer, loss):\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.compiled = True\n",
    "\n",
    "    def summary(self):\n",
    "        for layer in self.layers:\n",
    "            print(f\"Layer: {layer}\")\n",
    "\n",
    "model = SequentialModel()\n",
    "model.add(\"Dense Layer 1\")\n",
    "model.add(\"Dense Layer 2\")\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ddb972-9550-4bd2-9c1b-a1858ec5f813",
   "metadata": {},
   "source": [
    "14. Implement a function that visualizes the weights of a trained neural network (e.g., weights of the first convolutional layer in a CNN) as an image grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c51540-61b5-40d5-95ff-eafeba550d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_weights(model, layer_idx):\n",
    "    weights, biases = model.layers[layer_idx].get_weights()\n",
    "    weights = weights[:, :, :, 0]  # Assuming 1st filter\n",
    "    \n",
    "    fig, axs = plt.subplots(4, 8, figsize=(8, 8))\n",
    "    fig.suptitle('Convolutional Layer Weights Visualization')\n",
    "    for i in range(32):\n",
    "        ax = axs[i // 8, i % 8]\n",
    "        ax.imshow(weights[:, :, i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "visualize_weights(model, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057bbfc6-4241-4a22-a6ce-5efce3541959",
   "metadata": {},
   "source": [
    "15. Write code to implement a GRU from scratch, then use it to model a sequence prediction task such as predicting the next word in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f4c06b1-5ca3-43a0-a525-f9f5dd3a95cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0309454  -0.37395334  0.21597958  0.21007854 -0.16675797 -0.40692488\n",
      "  -0.70143874 -0.13474975]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GRUCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Initialize weights for update gate (z), reset gate (r), and new gate (h)\n",
    "        self.Wz = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.Wr = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.Wh = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        \n",
    "        # Initialize biases for each gate\n",
    "        self.bz = np.zeros((hidden_size, 1))\n",
    "        self.br = np.zeros((hidden_size, 1))\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        combined = np.concatenate((h_prev, x), axis=1).T\n",
    "        \n",
    "        z = self.sigmoid(np.dot(self.Wz, combined) + self.bz)\n",
    "        \n",
    "        r = self.sigmoid(np.dot(self.Wr, combined) + self.br)\n",
    "        \n",
    "        combined_r = np.concatenate((r * h_prev.T, x.T), axis=0)\n",
    "        \n",
    "        h_hat = np.tanh(np.dot(self.Wh, combined_r) + self.bh)\n",
    "        \n",
    "        h_next = (1 - z) * h_prev.T + z * h_hat\n",
    "        \n",
    "        return h_next.T  # Transpose the result back\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "input_size, hidden_size = 4, 8\n",
    "gru_cell = GRUCell(input_size, hidden_size)\n",
    "x = np.random.randn(1, input_size)\n",
    "h_prev = np.zeros((1, hidden_size))\n",
    "\n",
    "h_next = gru_cell.forward(x, h_prev)\n",
    "print(h_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240f67e-e9da-4426-b46b-876ce839644d",
   "metadata": {},
   "source": [
    "16. Write code to perform hyperparameter tuning using techniques like grid search or random search to find the best hyperparameters for a neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b87d544-dddc-4369-85d8-2ea06432eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def build_model(optimizer='adam'):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=build_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "param_grid = {\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [10, 20],\n",
    "    'optimizer': ['adam', 'rmsprop']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162f7fa-7267-44cf-bba0-7ce3423f4794",
   "metadata": {},
   "source": [
    "17. Write a code to add layers using keras API in the IRIS Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b34c3c5-332c-46bf-b5fe-c24d523751f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdabd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 153ms/step - accuracy: 0.5510 - loss: 1.0678 - val_accuracy: 0.6333 - val_loss: 0.9833\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6867 - loss: 0.8866 - val_accuracy: 0.6333 - val_loss: 0.8407\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6604 - loss: 0.8045 - val_accuracy: 0.7333 - val_loss: 0.7648\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5748 - loss: 0.7676 - val_accuracy: 0.7000 - val_loss: 0.7287\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6633 - loss: 0.7340 - val_accuracy: 0.7000 - val_loss: 0.7026\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6925 - loss: 0.7011 - val_accuracy: 0.7000 - val_loss: 0.6787\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6529 - loss: 0.6960 - val_accuracy: 0.7000 - val_loss: 0.6572\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6585 - loss: 0.6610 - val_accuracy: 0.8333 - val_loss: 0.6437\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7952 - loss: 0.6328 - val_accuracy: 0.8667 - val_loss: 0.6323\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8940 - loss: 0.6089 - val_accuracy: 0.9000 - val_loss: 0.6194\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8700 - loss: 0.6273 - val_accuracy: 0.9333 - val_loss: 0.6077\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8885 - loss: 0.6143 - val_accuracy: 0.9333 - val_loss: 0.5972\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8815 - loss: 0.6124 - val_accuracy: 0.9333 - val_loss: 0.5865\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8540 - loss: 0.5861 - val_accuracy: 0.9333 - val_loss: 0.5717\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9073 - loss: 0.5764 - val_accuracy: 0.9000 - val_loss: 0.5613\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9210 - loss: 0.5757 - val_accuracy: 0.8333 - val_loss: 0.5504\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8046 - loss: 0.5755 - val_accuracy: 0.9000 - val_loss: 0.5426\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8042 - loss: 0.5435 - val_accuracy: 0.8333 - val_loss: 0.5327\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8358 - loss: 0.5067 - val_accuracy: 0.8333 - val_loss: 0.5251\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8323 - loss: 0.5516 - val_accuracy: 0.8667 - val_loss: 0.5193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14453bd5af0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False) \n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372452fd-9a87-4e84-ade3-00695032222e",
   "metadata": {},
   "source": [
    "18. Implement the RNN using the IMDB Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbafd051-7853-4d73-b400-f74f227e49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "max_len = 500\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(max_features, 128, input_length=max_len),\n",
    "    tf.keras.layers.SimpleRNN(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=2, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41d2b8c-c3e2-4bf2-bfd3-e3c822ad9556",
   "metadata": {},
   "source": [
    "19. Write a code to fit ad compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c4108-6f09-40c4-845f-5a2a95d7547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfc0a5a-2db3-47da-acd0-1d3a3d3ac0d7",
   "metadata": {},
   "source": [
    "20. Implement a Long Short-Term Memory (LSTM) network to predict stock prices or any time-series data (e.g., temperature) using historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a76902-8c43-482a-b35a-9a415f3bd33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = pd.read_csv('Stock Prices.csv')\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data['Close'].values.reshape(-1, 1))\n",
    "\n",
    "sequence_length = 60\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(scaled_data, sequence_length)\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(50, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "    tf.keras.layers.LSTM(50),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X, y, epochs=10, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
